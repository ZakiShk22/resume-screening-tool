{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f42665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:90: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:90: SyntaxWarning: invalid escape sequence '\\/'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3344\\1284037006.py:90: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  text2 = re.sub('https:\\/\\/.*[\\r\\n]*','', text)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3344\\1284037006.py:90: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  text2 = re.sub('https:\\/\\/.*[\\r\\n]*','', text)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\app\\\\model\\\\media\\\\DURGESH BABU P.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 524\u001b[39m\n\u001b[32m    517\u001b[39m     ResumeAnalysisob=ResumeAnalysis()\n\u001b[32m    518\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.loaddataset()\u001b[39;00m\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.dataexploration()\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.wordvectorizer()\u001b[39;00m\n\u001b[32m    521\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.datatranformation()\u001b[39;00m\n\u001b[32m    522\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.Modeltraining()\u001b[39;00m\n\u001b[32m    523\u001b[39m     \u001b[38;5;66;03m#ResumeAnalysisob.rankall()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[43mcvanalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedia/DURGESH BABU P.pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedia/DURGESH BABU P.png\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m#     cvanalysis(os.path.abspath('media/Gagan Resume.pdf'),os.path.abspath('media/Gagan Resume.png'))\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m#     cvanalysis(os.path.abspath('media/Resume.pdf'),os.path.abspath('media/Resume.png'))\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 507\u001b[39m, in \u001b[36mcvanalysis\u001b[39m\u001b[34m(filepath, resultpath)\u001b[39m\n\u001b[32m    505\u001b[39m resumefilepath= filepath\n\u001b[32m    506\u001b[39m ResumeAnalysisob = ResumeAnalysis()\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m text= \u001b[43mResumeAnalysisob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvertresumefileToText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m     \u001b[38;5;66;03m#predictedclass= self.predicttext(text)\u001b[39;00m\n\u001b[32m    509\u001b[39m result=ResumeAnalysisob.scoreresume(text,resultpath)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 472\u001b[39m, in \u001b[36mResumeAnalysis.convertresumefileToText\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    470\u001b[39m     text = \u001b[38;5;28mself\u001b[39m.extract_text_from_docx(filepath)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fileExtension == \u001b[33m\"\u001b[39m\u001b[33mpdf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdftotext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 438\u001b[39m, in \u001b[36mResumeAnalysis.pdftotext\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpdftotext\u001b[39m(\u001b[38;5;28mself\u001b[39m,filepath):\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Open pdf file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     pdfFileObj = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Read file\u001b[39;00m\n\u001b[32m    441\u001b[39m     pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\app\\\\model\\\\media\\\\DURGESH BABU P.pdf'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from django.conf import settings\n",
    "# Import required libraries\n",
    "import PyPDF2\n",
    "import textract\n",
    "import docx2txt\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from django.conf import settings\n",
    "\n",
    "class ResumeAnalysis():\n",
    "    def __init__(self, resumedata=os.path.abspath('cvdata/UpdatedResumeDataSet.csv'),\n",
    "                 result=\"/content/drive/MyDrive/archive (1)/augmented_dataset/\",\n",
    "                 rank=\"/content/drive/MyDrive/Datafiles/\",\n",
    "                 models_path=os.path.abspath('model'),\n",
    "                 #resumefolder= settings.MEDIA_ROOT,\n",
    "                 jobapplicationfolder=os.path.abspath('jobs')):\n",
    "        self.resumedata= resumedata\n",
    "        self.result = result\n",
    "        self.rank = rank\n",
    "        self.models_path=models_path\n",
    "        #self.resumefolder=resumefolder\n",
    "\n",
    "        self.jobapplicationfolder=jobapplicationfolder\n",
    "\n",
    "        \"\"\"## Load dataset\"\"\"\n",
    "    def loaddataset(self):\n",
    "        df_CV = pd.read_csv(self.resumedata)\n",
    "        self.df_CV=df_CV\n",
    "\n",
    "        \"\"\"# Data exploration\"\"\"\n",
    "\n",
    "    def dataexploration(self):\n",
    "        df_CV=self.df_CV\n",
    "        # Shape of dataset\n",
    "        df_CV.head(10)\n",
    "        df_CV.shape\n",
    "\n",
    "        # Missing values\n",
    "\n",
    "        print(df_CV.isnull().sum())\n",
    "\n",
    "        df_CV.describe()\n",
    "\n",
    "        df_CV.info()\n",
    "\n",
    "        df_CV['Category'].unique()\n",
    "\n",
    "        # Category unique value\n",
    "\n",
    "        print(\"Category :\\n\", df_CV['Category'].unique())\n",
    "        print(\"\\nThe are \",len(df_CV['Category'].value_counts()),\"diffrents values\" )\n",
    "\n",
    "        # Categroy repartition\n",
    "\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.xticks(rotation=90)\n",
    "        sns.countplot(x = \"Category\", data = df_CV)\n",
    "\n",
    "        category = df_CV['Category'].value_counts().reset_index()\n",
    "        category\n",
    "\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.pie(category['Category'], labels=category['index'],\n",
    "                colors=sns.color_palette('tab20'), autopct='%1.1f%%')\n",
    "        plt.title('Category Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        df_CV['Category'].value_counts()\n",
    "\n",
    "    \"\"\"# Data Cleaning\"\"\"\n",
    "    def process_text(self,text,stopwords_english):\n",
    "        text2 = re.sub('https:\\/\\/.*[\\r\\n]*','', text)\n",
    "        text3 = re.sub(r'[^\\w\\s]','', text2)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        text_mots = tokenizer.tokenize(text3)\n",
    "        text_mots_lower = [word.lower() for word in text_mots]\n",
    "        text_clean = [word for word in text_mots_lower if (word not in stopwords_english and len(word)>2)]\n",
    "        return text_clean\n",
    "\n",
    "    def dataexploration(self):\n",
    "\n",
    "        ### Remove stopwords, punctuation and tokenize text\n",
    "        stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        df_CV = self.df_CV\n",
    "        df_CV[\"CV_clean\"] = \"\"\n",
    "\n",
    "        df_CV['CV_clean'] = df_CV['Resume'].apply(lambda x:self.process_text(x,stopwords_english))\n",
    "        df_CV.head()\n",
    "        self.df_CVclean = df_CV\n",
    "\n",
    "    \"\"\"# Data Visualization\"\"\"\n",
    "    def datavisualization(self):\n",
    "        # Datascience words frequency\n",
    "        df_CV = self.df_CVclean\n",
    "        from nltk import FreqDist\n",
    "        l = list(df_CV[df_CV['Category'] == \"Data Science\"]['CV_clean'])\n",
    "        datascience_mots = [item for sublist in l for item in sublist]\n",
    "        rec_datascience_mots = FreqDist(datascience_mots)\n",
    "\n",
    "        sns.set()\n",
    "        ax = plt.figure(figsize = (15,10))\n",
    "        rec_datascience_mots.plot(25)\n",
    "\n",
    "        wordcloud = WordCloud(width = 1000, height = 500).generate(\",\".join(str(v) for v in rec_datascience_mots))\n",
    "\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Wordcloud Data Science CV')\n",
    "\n",
    "        # Sales words frequency\n",
    "\n",
    "        from nltk import FreqDist\n",
    "        l = list(df_CV[df_CV['Category'] == \"Sales\"]['CV_clean'])\n",
    "        sales_mots = [item for sublist in l for item in sublist]\n",
    "        rec_sales_mots = FreqDist(sales_mots)\n",
    "\n",
    "        sns.set()\n",
    "        ax = plt.figure(figsize = (15,10))\n",
    "        rec_sales_mots.plot(25)\n",
    "\n",
    "        wordcloud = WordCloud(width = 1000, height = 500).generate(\",\".join(str(v) for v in rec_sales_mots))\n",
    "\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Wordcloud sales CV')\n",
    "\n",
    "        # SAP Developer words frequency\n",
    "\n",
    "        from nltk import FreqDist\n",
    "        l = list(df_CV[df_CV['Category'] == \"SAP Developer\"]['CV_clean'])\n",
    "        sap_mots = [item for sublist in l for item in sublist]\n",
    "        rec_sap_mots = FreqDist(sap_mots)\n",
    "\n",
    "        sns.set()\n",
    "        ax = plt.figure(figsize = (15,10))\n",
    "        rec_sap_mots.plot(25)\n",
    "\n",
    "        wordcloud = WordCloud(width = 1000, height = 500).generate(\",\".join(str(v) for v in rec_sap_mots))\n",
    "\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Wordcloud sap CV')\n",
    "\n",
    "    \"\"\"# Machine Learning\"\"\"\n",
    "    def wordvectorizer(self):\n",
    "        #Word vectorizer\n",
    "        df_CV = self.df_CVclean\n",
    "        df_ml = df_CV\n",
    "        df_ml['CV_clean']\n",
    "\n",
    "        df_ml.dtypes\n",
    "\n",
    "        df_ml['CV_clean']=df_ml['CV_clean'].apply(lambda x:' '.join(x))\n",
    "        df_ml\n",
    "        requiredTarget = df_ml['Category'].values\n",
    "        requiredText = df_ml['CV_clean'].values\n",
    "\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            stop_words='english',\n",
    "            max_features=266)\n",
    "        word_vectorizer.fit(requiredText)\n",
    "        WordFeatures = word_vectorizer.transform(requiredText)\n",
    "        print(WordFeatures)\n",
    "        self.df_CVml = df_ml\n",
    "        self.requiredTarget = requiredTarget\n",
    "        self.WordFeatures = WordFeatures\n",
    "\n",
    "    #Train-Test\n",
    "    def datatranformation(self):\n",
    "        requiredTarget=self.requiredTarget\n",
    "        WordFeatures=self.WordFeatures\n",
    "        X_train, X_test, y_train, y_test = train_test_split(WordFeatures,requiredTarget,random_state=42, test_size=0.2,\n",
    "                                                         shuffle=True, stratify=requiredTarget)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    #Prediction Model\n",
    "    def Modeltraining(self):\n",
    "        X_train = self.X_train\n",
    "        X_test = self.X_test\n",
    "        y_train = self.y_train\n",
    "        y_test = self.y_test\n",
    "        model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "        model.fit(X_train, y_train)\n",
    "        self.model=model\n",
    "        prediction = model.predict(X_test)\n",
    "        self.prediction=prediction\n",
    "        #Accuracy\n",
    "        print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(model.score(X_train, y_train)))\n",
    "        print('_________________________________________________________\\n')\n",
    "        print('Accuracy of KNeighbors Classifier on test set:     {:.2f}'.format(model.score(X_test, y_test)))\n",
    "        prediction\n",
    "        print(\"\\n Classification report for classifier %s:\\n%s\\n\" % (model, metrics.classification_report(y_test, prediction)))\n",
    "        joblib.dump(model, self.models_path + \"/model_knn.sav\")\n",
    "\n",
    "    def predicttext(self,text):\n",
    "        # load the model from disk\n",
    "        stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "                             \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "                             \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "                             \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\",\n",
    "                             \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "                             \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\",\n",
    "                             \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "                             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "                             \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
    "                             \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
    "                             \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\",\n",
    "                             \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "        text = text.apply(lambda x: self.process_text(x, stopwords_english))\n",
    "        text = text.apply(lambda x: ' '.join(x))\n",
    "        loaded_model = joblib.load(self.models_path+ \"/model_knn.sav\")\n",
    "        input=np.array(text)\n",
    "        result = loaded_model.predict(input)\n",
    "        return result\n",
    "\n",
    "    def scoreresume(self,resumetext,resultpath):\n",
    "        # Convert all strings to lowercase\n",
    "        text = resumetext.lower()\n",
    "\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Create dictionary with industrial and system engineering key terms by area\n",
    "        terms = {'Quality/Six Sigma': ['black belt', 'capability analysis', 'control charts', 'doe', 'dmaic', 'fishbone',\n",
    "                                       'gage r&r', 'green belt', 'ishikawa', 'iso', 'kaizen', 'kpi', 'lean', 'metrics',\n",
    "                                       'pdsa', 'performance improvement', 'process improvement', 'quality',\n",
    "                                       'quality circles', 'quality tools', 'root cause', 'six sigma',\n",
    "                                       'stability analysis', 'statistical analysis', 'tqm'],\n",
    "                 'Operations management': ['automation', 'bottleneck', 'constraints', 'cycle time', 'efficiency', 'fmea',\n",
    "                                           'machinery', 'maintenance', 'manufacture', 'line balancing', 'oee', 'operations',\n",
    "                                           'operations research', 'optimization', 'overall equipment effectiveness',\n",
    "                                           'pfmea', 'process', 'process mapping', 'production', 'resources', 'safety',\n",
    "                                           'stoppage', 'value stream mapping', 'utilization'],\n",
    "                 'Supply chain': ['abc analysis', 'apics', 'customer', 'customs', 'delivery', 'distribution', 'eoq', 'epq',\n",
    "                                  'fleet', 'forecast', 'inventory', 'logistic', 'materials', 'outsourcing', 'procurement',\n",
    "                                  'reorder point', 'rout', 'safety stock', 'scheduling', 'shipping', 'stock', 'suppliers',\n",
    "                                  'third party logistics', 'transport', 'transportation', 'traffic', 'supply chain',\n",
    "                                  'vendor', 'warehouse', 'wip', 'work in progress'],\n",
    "                 'Project management': ['administration', 'agile', 'budget', 'cost', 'direction', 'feasibility analysis',\n",
    "                                        'finance', 'kanban', 'leader', 'leadership', 'management', 'milestones', 'planning',\n",
    "                                        'pmi', 'pmp', 'problem', 'project', 'risk', 'schedule', 'scrum', 'stakeholders'],\n",
    "                 'Data analytics': ['analytics', 'api', 'aws', 'big data', 'business intelligence', 'clustering', 'code',\n",
    "                                    'coding', 'data', 'database', 'data mining', 'data science', 'deep learning', 'hadoop',\n",
    "                                    'hypothesis test', 'iot', 'internet', 'machine learning', 'modeling', 'nosql', 'nlp',\n",
    "                                    'predictive', 'programming', 'python', 'r', 'sql', 'tableau', 'text mining',\n",
    "                                    'visualuzation'],\n",
    "                 'Healthcare': ['adverse events', 'care', 'clinic', 'cphq', 'ergonomics', 'healthcare',\n",
    "                                'health care', 'health', 'hospital', 'human factors', 'medical', 'near misses',\n",
    "                                'patient', 'reporting system']}\n",
    "\n",
    "        # Initializie score counters for each area\n",
    "        quality = 0\n",
    "        operations = 0\n",
    "        supplychain = 0\n",
    "        project = 0\n",
    "        data = 0\n",
    "        healthcare = 0\n",
    "\n",
    "        # Create an empty list where the scores will be stored\n",
    "        scores = []\n",
    "\n",
    "        # Obtain the scores for each area\n",
    "        for area in terms.keys():\n",
    "\n",
    "            if area == 'Quality/Six Sigma':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        quality += 1\n",
    "                scores.append(quality)\n",
    "\n",
    "            elif area == 'Operations management':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        operations += 1\n",
    "                scores.append(operations)\n",
    "\n",
    "            elif area == 'Supply chain':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        supplychain += 1\n",
    "                scores.append(supplychain)\n",
    "\n",
    "            elif area == 'Project management':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        project += 1\n",
    "                scores.append(project)\n",
    "\n",
    "            elif area == 'Data analytics':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        data += 1\n",
    "                scores.append(data)\n",
    "\n",
    "            else:\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        healthcare += 1\n",
    "                scores.append(healthcare)\n",
    "\n",
    "        # Create a data frame with the scores summary\n",
    "        summary = pd.DataFrame(scores, index=terms.keys(), columns=['score']).sort_values(by='score', ascending=False)\n",
    "        summary\n",
    "\n",
    "        # Create pie chart visualization\n",
    "        pie = plt.figure(figsize=(10, 10))\n",
    "        plt.pie(summary['score'], labels=summary.index, explode=(0.1, 0, 0, 0, 0, 0), autopct='%1.0f%%', shadow=True,\n",
    "               startangle=90)\n",
    "        plt.title('Candidate - Resume Decomposition by Areas')\n",
    "        plt.axis('equal')\n",
    "\n",
    "        #plt.show()\n",
    "\n",
    "        # Save pie chart as a .png file\n",
    "        pie.savefig(resultpath)\n",
    "        return summary['score']\n",
    "\n",
    "    def scoreresumeall(self, resumetext):\n",
    "        # Convert all strings to lowercase\n",
    "        text = resumetext.lower()\n",
    "\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Create dictionary with industrial and system engineering key terms by area\n",
    "        terms = {\n",
    "            'Quality/Six Sigma': ['black belt', 'capability analysis', 'control charts', 'doe', 'dmaic', 'fishbone',\n",
    "                                  'gage r&r', 'green belt', 'ishikawa', 'iso', 'kaizen', 'kpi', 'lean', 'metrics',\n",
    "                                  'pdsa', 'performance improvement', 'process improvement', 'quality',\n",
    "                                  'quality circles', 'quality tools', 'root cause', 'six sigma',\n",
    "                                  'stability analysis', 'statistical analysis', 'tqm'],\n",
    "            'Operations management': ['automation', 'bottleneck', 'constraints', 'cycle time', 'efficiency', 'fmea',\n",
    "                                      'machinery', 'maintenance', 'manufacture', 'line balancing', 'oee', 'operations',\n",
    "                                      'operations research', 'optimization', 'overall equipment effectiveness',\n",
    "                                      'pfmea', 'process', 'process mapping', 'production', 'resources', 'safety',\n",
    "                                      'stoppage', 'value stream mapping', 'utilization'],\n",
    "            'Supply chain': ['abc analysis', 'apics', 'customer', 'customs', 'delivery', 'distribution', 'eoq', 'epq',\n",
    "                             'fleet', 'forecast', 'inventory', 'logistic', 'materials', 'outsourcing', 'procurement',\n",
    "                             'reorder point', 'rout', 'safety stock', 'scheduling', 'shipping', 'stock', 'suppliers',\n",
    "                             'third party logistics', 'transport', 'transportation', 'traffic', 'supply chain',\n",
    "                             'vendor', 'warehouse', 'wip', 'work in progress'],\n",
    "            'Project management': ['administration', 'agile', 'budget', 'cost', 'direction', 'feasibility analysis',\n",
    "                                   'finance', 'kanban', 'leader', 'leadership', 'management', 'milestones', 'planning',\n",
    "                                   'pmi', 'pmp', 'problem', 'project', 'risk', 'schedule', 'scrum', 'stakeholders'],\n",
    "            'Data analytics': ['analytics', 'api', 'aws', 'big data', 'business intelligence', 'clustering', 'code',\n",
    "                               'coding', 'data', 'database', 'data mining', 'data science', 'deep learning', 'hadoop',\n",
    "                               'hypothesis test', 'iot', 'internet', 'machine learning', 'modeling', 'nosql', 'nlp',\n",
    "                               'predictive', 'programming', 'python', 'r', 'sql', 'tableau', 'text mining',\n",
    "                               'visualuzation'],\n",
    "            'Healthcare': ['adverse events', 'care', 'clinic', 'cphq', 'ergonomics', 'healthcare',\n",
    "                           'health care', 'health', 'hospital', 'human factors', 'medical', 'near misses',\n",
    "                           'patient', 'reporting system']}\n",
    "\n",
    "        # Initializie score counters for each area\n",
    "        quality = 0\n",
    "        operations = 0\n",
    "        supplychain = 0\n",
    "        project = 0\n",
    "        data = 0\n",
    "        healthcare = 0\n",
    "\n",
    "        # Create an empty list where the scores will be stored\n",
    "        scores = []\n",
    "\n",
    "        # Obtain the scores for each area\n",
    "        for area in terms.keys():\n",
    "\n",
    "            if area == 'Quality/Six Sigma':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        quality += 1\n",
    "                scores.append(quality)\n",
    "\n",
    "            elif area == 'Operations management':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        operations += 1\n",
    "                scores.append(operations)\n",
    "\n",
    "            elif area == 'Supply chain':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        supplychain += 1\n",
    "                scores.append(supplychain)\n",
    "\n",
    "            elif area == 'Project management':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        project += 1\n",
    "                scores.append(project)\n",
    "\n",
    "            elif area == 'Data analytics':\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        data += 1\n",
    "                scores.append(data)\n",
    "\n",
    "            else:\n",
    "                for word in terms[area]:\n",
    "                    if word in text:\n",
    "                        healthcare += 1\n",
    "                scores.append(healthcare)\n",
    "\n",
    "        # Create a data frame with the scores summary\n",
    "        summary = pd.DataFrame(scores, index=terms.keys(), columns=['score']).sort_values(by='score', ascending=False)\n",
    "        summary\n",
    "\n",
    "        return summary['score']\n",
    "    def pdftotext(self,filepath):\n",
    "        # Open pdf file\n",
    "        pdfFileObj = open(filepath, 'rb')\n",
    "\n",
    "        # Read file\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "        # Get total number of pages\n",
    "        num_pages = pdfReader.numPages\n",
    "\n",
    "        # Initialize a count for the number of pages\n",
    "        count = 0\n",
    "\n",
    "        # Initialize a text empty etring variable\n",
    "        text = \"\"\n",
    "\n",
    "        # Extract text from every page on the file\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count += 1\n",
    "            text += pageObj.extractText()\n",
    "        return text\n",
    "\n",
    "\n",
    "    def extract_text_from_docx(self,docx_path):\n",
    "        txt = docx2txt.process(docx_path)\n",
    "        if txt:\n",
    "            return txt.replace('\\t', ' ')\n",
    "        return None\n",
    "\n",
    "    def convertresumefileToText(self,filepath):\n",
    "        text =\" \"\n",
    "        fileExtension = filepath.split(\".\")[-1]\n",
    "        if fileExtension == \"docx\":\n",
    "            text = self.extract_text_from_docx(filepath)\n",
    "        elif fileExtension == \"pdf\":\n",
    "            text = self.pdftotext(filepath)\n",
    "        return text\n",
    "\n",
    "def rankall(ROOT,applicantnames):\n",
    "    ResumeAnalysisob = ResumeAnalysis()\n",
    "    resumefilepath= ROOT\n",
    "    cvsresults=pd.DataFrame(columns=['Resumepath','Data analytics','Quality/Six Sigma','Operations management','Supply chain','Project management','Healthcare'])\n",
    "    #result=pd.DataFrame()\n",
    "    columns=['Data analytics', 'Quality/Six Sigma', 'Operations management', 'Supply chain', 'Project management', 'Healthcare']\n",
    "    files=applicantnames\n",
    "    for i in range(0,len(files)):\n",
    "        print(files[i]+'.pdf')\n",
    "        text= ResumeAnalysisob.convertresumefileToText(resumefilepath+files[i]+'.pdf')\n",
    "        #predictedclass= self.predicttext(text)\n",
    "        result=ResumeAnalysisob.scoreresumeall(text)\n",
    "        categories=list(result.index)\n",
    "        cvscat = dict(result)\n",
    "        catresults=[]\n",
    "        for j in range(0,len(columns)):\n",
    "            for category in categories:\n",
    "                if columns[j] == category:\n",
    "                    result=cvscat[category]\n",
    "                    catresults.append(result)\n",
    "        print(catresults)\n",
    "        cvsresults.loc[i]=[files[i]]+catresults\n",
    "        #for j in range(0,len(categories)):\n",
    "        #    cvscat.loc[i]=[result[categories[j]]]\n",
    "    print(cvsresults)\n",
    "    #resultcategory1 = result.sort_values('Courses', ascending=False)\n",
    "    #cvsresults.to_csv( resumefilepath+ 'cvinfo.csv')\n",
    "    return cvsresults\n",
    "\n",
    "def cvanalysis(filepath,resultpath):\n",
    "    resumefilepath= filepath\n",
    "    ResumeAnalysisob = ResumeAnalysis()\n",
    "    text= ResumeAnalysisob.convertresumefileToText(filepath)\n",
    "        #predictedclass= self.predicttext(text)\n",
    "    result=ResumeAnalysisob.scoreresume(text,resultpath)\n",
    "    Resumepath=filepath\n",
    "    CV=text\n",
    "    Categories=list(result.index)\n",
    "    Score=result\n",
    "    return Resumepath,CV,Categories,Score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ResumeAnalysisob=ResumeAnalysis()\n",
    "    #ResumeAnalysisob.loaddataset()\n",
    "    #ResumeAnalysisob.dataexploration()\n",
    "    #ResumeAnalysisob.wordvectorizer()\n",
    "    #ResumeAnalysisob.datatranformation()\n",
    "    #ResumeAnalysisob.Modeltraining()\n",
    "    #ResumeAnalysisob.rankall()\n",
    "    cvanalysis(os.path.abspath('media/DURGESH BABU P.pdf'),os.path.abspath('media/DURGESH BABU P.png'))\n",
    "#     cvanalysis(os.path.abspath('media/Gagan Resume.pdf'),os.path.abspath('media/Gagan Resume.png'))\n",
    "#     cvanalysis(os.path.abspath('media/Resume.pdf'),os.path.abspath('media/Resume.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3140273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
